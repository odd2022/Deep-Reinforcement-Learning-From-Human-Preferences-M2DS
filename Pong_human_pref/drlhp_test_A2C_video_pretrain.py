import numpy as np
import cv2
from collections import deque
from itertools import combinations
from random import shuffle
import os
import gymnasium as gym
import ale_py
import torch
import torch.nn as nn
import torch.optim as optim
from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import DummyVecEnv
from drlhp_train_A2C_video import HumanPreferencesEnvWrapper

# Nom du fichier pour sauvegarder le mod√®le de r√©compense
REWARD_MODEL_PATH = "reward_predictor.pth"

# Cr√©er l'environnement
env = DummyVecEnv([lambda: HumanPreferencesEnvWrapper(
    gym.make("Pong-v4"), segment_length=20, synthetic_prefs=False, log_dir="logs/"
)])

# Collecter des segments initiaux pour le pr√©-entra√Ænement
def collect_initial_segments(env, num_steps=1000):
    """Permet de collecter des segments initiaux en jouant de mani√®re al√©atoire."""
    print(f"üé• Collecte de {num_steps} √©tapes de jeu pour initialiser les segments...")

    obs = env.reset()
    for _ in range(num_steps):
        action = env.action_space.sample()  # Actions al√©atoires pour explorer
        env.step(action)  # Ex√©cute l'action et collecte les donn√©es

    print(f"‚úÖ {num_steps} √©tapes collect√©es. Pr√™t pour le pr√©-entra√Ænement !")


# Charger le mod√®le de pr√©dicteur de r√©compense s'il existe
def load_reward_predictor(env, model_path=REWARD_MODEL_PATH):
    if os.path.exists(model_path):
        env.get_attr("reward_predictor")[0].load_state_dict(torch.load(model_path))
        print(f"‚úÖ Mod√®le pr√©-entra√Æn√© charg√© depuis {model_path}")
    else:
        print("‚ö†Ô∏è Aucun mod√®le pr√©-entra√Æn√© trouv√©, entra√Ænement depuis z√©ro.")

# Phase de pr√©-entra√Ænement du pr√©dicteur de r√©compense
def pretrain_reward_predictor(env, num_pretrain_steps=10):
    print("üîπ D√©but du pr√©-entra√Ænement du pr√©dicteur de r√©compense...")

    pref_interface = env.get_attr("pref_interface")[0]
    reward_predictor = env.get_attr("reward_predictor")[0]
    optimizer = env.get_attr("optimizer")[0]
    criterion = env.get_attr("criterion")[0]

    for step in range(num_pretrain_steps):
        pref_result = pref_interface.query_user()
        if pref_result:
            s1, s2, preference = pref_result
            loss = reward_predictor.train_model(s1, s2, preference, optimizer, criterion)
            print(f"üîÑ Pr√©-entra√Ænement Step {step + 1}/{num_pretrain_steps} - Loss: {loss}")

    print("‚úÖ Pr√©-entra√Ænement termin√© !")

    # Sauvegarde du mod√®le pr√©-entra√Æn√©
    torch.save(reward_predictor.state_dict(), REWARD_MODEL_PATH)
    print(f"‚úÖ Mod√®le pr√©-entra√Æn√© sauvegard√© sous {REWARD_MODEL_PATH}")

# Charger un mod√®le existant si disponible
load_reward_predictor(env)

# Effectuer le pr√©-entra√Ænement avant l'entra√Ænement du mod√®le A2C
# Avant le pr√©-entra√Ænement, on collecte des donn√©es
collect_initial_segments(env, num_steps=1000)
pretrain_reward_predictor(env, num_pretrain_steps=10)

# Cr√©ation du mod√®le A2C
model = A2C("MlpPolicy", env, verbose=1)

# Nombre d'√©pisodes d'entra√Ænement
num_episodes = 50

# Boucle d'entra√Ænement principale
for episode in range(num_episodes):
    obs = env.reset()
    done = False
    while not done:
        action, _ = model.predict(obs)
        step_result = env.step(action)

        if len(action) == 5:
            obs, reward, terminated, truncated, info = step_result
        else:
            obs, reward, done, info = step_result
            terminated, truncated = done, False

    # Passage aux r√©compenses apprises √† mi-chemin
    if episode == num_episodes // 2:
        env.get_attr("switch_to_predicted_reward")[0]()
        print("üéØ Passage aux r√©compenses pr√©dites √† mi-entra√Ænement.")

    # Comparaison et apprentissage du pr√©dicteur de r√©compense
    pref_interface = env.get_attr("pref_interface")[0]
    pref_result = pref_interface.query_user()

    if pref_result:
        s1, s2, preference = pref_result
        loss = env.get_attr("reward_predictor")[0].train_model(
            s1, s2, preference, env.get_attr("optimizer")[0], env.get_attr("criterion")[0]
        )
        print(f"üìâ Loss du mod√®le de r√©compense : {loss}")

    model.learn(total_timesteps=500)

print("üèÜ Entra√Ænement termin√© avec A2C !")
